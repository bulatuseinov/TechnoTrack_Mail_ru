{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import random\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEVICE_ID = 0\n",
    "DEVICE = torch.device('cuda:%d' % DEVICE_ID)\n",
    "torch.cuda.set_device(DEVICE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Для запуска без GPU раскомментировать и закоментировать код выше\n",
    "# DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(100500)\n",
    "\n",
    "def data2image(data):\n",
    "    res = np.transpose(np.reshape(data ,(3, 32,32)), (1,2,0))\n",
    "    return PIL.Image.fromarray(np.uint8(res))\n",
    "\n",
    "def imshow(img):\n",
    "    if isinstance(img, torch.Tensor): img = img.numpy().astype('uint8')\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    \n",
    "def prediction2classes(output_var):\n",
    "    _, predicted = torch.max(output_var.data, 1)\n",
    "    predicted.squeeze_()\n",
    "    classes = predicted.tolist()\n",
    "    return classes\n",
    "\n",
    "def make_solution_pytorch(net, input_tensor, a_batch_size):\n",
    "    res = []\n",
    "    net = net.eval()\n",
    "    cur_pos = 0\n",
    "    while cur_pos <= len(input_tensor):\n",
    "        outputs = net(input_tensor[cur_pos:cur_pos+a_batch_size])\n",
    "        res += prediction2classes(outputs)\n",
    "        cur_pos += a_batch_size\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "\n",
    "class CifarDataset(Dataset):\n",
    "    def __init__(self, input_path, is_train=True, transform=None):\n",
    "                        \n",
    "        data = np.load(input_path)\n",
    "        if is_train: \n",
    "            self.Y, self.X = np.hsplit(data, [1]) \n",
    "            self.Y = [item[0] for item in self.Y]\n",
    "        else: \n",
    "            self.X = data\n",
    "            self.Y = None\n",
    "            \n",
    "        self.X = self.X.reshape((self.X.shape[0], 3, 32, 32))\n",
    "        self.X = self.X.transpose((0, 2, 3, 1)) #приводим к виду (N, H, W, C)\n",
    "        self.X = [Image.fromarray(img) for img in self.X]\n",
    "                \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sample = self.X[idx]\n",
    "\n",
    "        if self.transform: sample = self.transform(sample)\n",
    "\n",
    "        if self.Y is None: return sample\n",
    "        else: return (sample, self.Y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Надо поменять пути на свои до файлов с kaggle\n",
    "DATA_PATH  = '/home/b.useinov/Networks_TT/04/'\n",
    "train_path = 'homework_4.train.npy'\n",
    "test_path  = 'homework_4_no_classes.test.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_mean = np.mean([item[0].numpy() for item in CifarDataset(DATA_PATH + train_path, transform=transforms.ToTensor())], axis=(0,2,3))\n",
    "np_std = np.std([item[0].numpy() for item in CifarDataset(DATA_PATH + train_path, transform=transforms.ToTensor())], axis=(0,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cifar_transform_norm = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.FloatTensor(np_mean), torch.FloatTensor(np_std))\n",
    "]\n",
    ")\n",
    "\n",
    "cifar_test_transform_norm = transforms.Compose([    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.FloatTensor(np_mean), torch.FloatTensor(np_std))\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_norm = CifarDataset(DATA_PATH + train_path, transform=cifar_transform_norm)\n",
    "dataloader_train_norm = DataLoader(dataset_train_norm, batch_size=128,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "dataset_test_norm = CifarDataset(DATA_PATH + test_path, is_train=False, transform=cifar_test_transform_norm)\n",
    "dataloader_test_norm = DataLoader(dataset_test_norm, batch_size=128,\n",
    "                        shuffle=False, num_workers=1)\n",
    "\n",
    "\n",
    "def train_network(a_net, \n",
    "                  a_device,\n",
    "                  dataloader_train_norm=dataloader_train_norm,\n",
    "                  a_epochs=164,\n",
    "                  a_batch_size=128,\n",
    "                  a_lr=0.1):\n",
    "    \n",
    "    train_acc = []\n",
    "    net = a_net.to(a_device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(a_net.parameters(), lr=a_lr, weight_decay=0.0001, momentum=0.9)\n",
    "\n",
    "    \n",
    "    for iter in tqdm.tqdm_notebook(range(a_epochs)):  # loop over the dataset multiple times\n",
    "        epoch = iter\n",
    "        if epoch == 82:\n",
    "            optimizer = torch.optim.SGD(a_net.parameters(), lr=a_lr/10, weight_decay=0.0001, momentum=0.9) \n",
    "        elif epoch == 123:\n",
    "            optimizer = torch.optim.SGD(a_net.parameters(), lr=a_lr/100, weight_decay=0.0001, momentum=0.99) \n",
    "        \n",
    "        net = net.train()        \n",
    "        epoch_accuracy = 0.0\n",
    "        epoch_iters = 0\n",
    "        for item in dataloader_train_norm:\n",
    "            \n",
    "            epoch_iters += 1\n",
    "\n",
    "            inputs = item[0].to(a_device)\n",
    "            labels = item[1].long().to(a_device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_accuracy += accuracy_score(labels, prediction2classes(outputs))\n",
    "\n",
    "        epoch_accuracy /= epoch_iters\n",
    "        train_acc.append(epoch_accuracy)\n",
    "        \n",
    "        print(\"Epoch \", epoch, round(train_acc[-1], 4))\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    plt.plot(train_acc, label='Train')\n",
    "    plt.legend()\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StupidDenseNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(StupidDenseNet, self).__init__()\n",
    "        \n",
    "        #Один из способов задать сеть - это задать последовательность слоев через Sequential\n",
    "        self.classifier = nn.Sequential()\n",
    "        self.classifier.add_module('lin1', nn.Linear(3*32*32, 500))\n",
    "        self.classifier.add_module('sig1', torch.nn.Sigmoid())\n",
    "        self.classifier.add_module('lin2', nn.Linear(3*32*32, 500))\n",
    "        self.classifier.add_module('sig2', torch.nn.Sigmoid())\n",
    "        self.classifier.add_module('lin3', nn.Linear(3*32*32, 500))\n",
    "        self.classifier.add_module('sig3', torch.nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        ### Другой способ задания сети - это описать слои и в forward их применять явно\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        #Увеличиваем кол-во выходных слоев с 84 - до 84*2 - потому что классов 100\n",
    "        self.fc2 = nn.Linear(120, 84*2)\n",
    "        self.fc3 = nn.Linear(84*2, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CifarResidualBlock(nn.Module):\n",
    "    def __init__(self, a_in_channels, make_downsample=False, use_skip_connection=True):\n",
    "        super(CifarResidualBlock, self).__init__()\n",
    "        self.use_skip_connection = use_skip_connection\n",
    "        \n",
    "        if make_downsample: coef = DOWNSAMPLE_COEF\n",
    "        else: coef = 1  \n",
    "            \n",
    "        ### TODO - нужно описать используемые блоки\n",
    "        self.conv_1 = nn.Conv2d(a_in_channels,\n",
    "                                a_in_channels * coef,\n",
    "                                kernel_size=3,\n",
    "                                stride=1*coef,\n",
    "                                padding=1,\n",
    "                                bias=False) \n",
    "        self.BatchNorm_1 = nn.BatchNorm2d(coef * a_in_channels, affine = False)\n",
    "        self.ReLU_1 = nn.PReLU()\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(a_in_channels * coef,\n",
    "                                a_in_channels * coef,\n",
    "                                kernel_size=3,\n",
    "                                stride=1,\n",
    "                                padding=1,\n",
    "                                bias=False)\n",
    "        self.BatchNorm_2 = nn.BatchNorm2d(coef * a_in_channels, affine = False)\n",
    "        self.ReLU_2 = nn.PReLU()\n",
    "        \n",
    "        self.downsample = x_downsample(a_in_channels)\n",
    "        self.make_downsample = make_downsample\n",
    "        self.use_skip_connection = use_skip_connection\n",
    "            \n",
    "    def forward(self, x):\n",
    "        ###TODO - описать forward блок с учетом флагов make_downsample и use_skip_connection\n",
    "\n",
    "        if self.use_skip_connection:\n",
    "            if self.make_downsample:\n",
    "                y = self.downsample(x)\n",
    "            else: y = x\n",
    "                \n",
    "        x = self.conv_1(x)\n",
    "        x = self.BatchNorm_1(x)\n",
    "        x = self.ReLU_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.BatchNorm_2(x)\n",
    "        \n",
    "        if self.use_skip_connection: x = x + y\n",
    "        \n",
    "        x = self.ReLU_2(x)\n",
    "             \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CifarResNet, self).__init__()\n",
    "        \n",
    "        #TODO нужно добавить блоков resnet и других слоев при необходимости\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.features = nn.Sequential()\n",
    "        \n",
    "        #реализуем классическую сеть реснет \n",
    "        \n",
    "        #self.features.add_module('MaxPool', nn.MaxPool2d((3,3), 2))\n",
    "        \n",
    "        self.features.add_module('3x3', nn.Conv2d(3, 16, 3, padding = 1))\n",
    "        \n",
    "        self.features.add_module('res_1_1', CifarResidualBlock(16))\n",
    "        self.features.add_module('res_1_2', CifarResidualBlock(16))\n",
    "        self.features.add_module('res_1_3', CifarResidualBlock(16))\n",
    "        self.features.add_module('ds_1', CifarResidualBlock(16, make_downsample = True))\n",
    "        \n",
    "        self.features.add_module('res_2_1', CifarResidualBlock(32))\n",
    "        self.features.add_module('res_2_2', CifarResidualBlock(32))\n",
    "        self.features.add_module('res_2_3', CifarResidualBlock(32))\n",
    "        self.features.add_module('ds_2', CifarResidualBlock(32, make_downsample = True))\n",
    "        \n",
    "        self.features.add_module('res_3_1', CifarResidualBlock(64))\n",
    "        self.features.add_module('res_3_2', CifarResidualBlock(64))\n",
    "        self.features.add_module('res_3_3', CifarResidualBlock(64))\n",
    "        self.features.add_module('ds_3', CifarResidualBlock(64, make_downsample = True))\n",
    "        \n",
    "        self.features.add_module('res_4_1', CifarResidualBlock(128))\n",
    "        self.features.add_module('res_4_2', CifarResidualBlock(128))\n",
    "        self.features.add_module('res_4_3', CifarResidualBlock(128))\n",
    "    \n",
    "        \n",
    "        \n",
    "        self.global_avg_pooling = nn.AvgPool2d(kernel_size=4)\n",
    "        self.fc_classifier = nn.Linear(128, 100)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.global_avg_pooling(x)        \n",
    "        x = x.view((x.size()[0], -1))        \n",
    "        x = self.fc_classifier(x)        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dense_net = StupidDenseNet()\n",
    "#%time train_network(dense_net, torch.device(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lenet = LeNet()\n",
    "#%time train_network(lenet, torch.device('cpu'), a_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lenet = LeNet()\n",
    "#%time train_network(lenet, torch.device(DEVICE), a_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lenet = LeNet()\n",
    "#%time train_network(lenet, torch.device(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 0.0852\n",
      "Epoch  1 0.1886\n",
      "Epoch  2 0.2645\n",
      "Epoch  3 0.3294\n",
      "Epoch  4 0.3798\n",
      "Epoch  5 0.4168\n",
      "Epoch  6 0.4532\n",
      "Epoch  7 0.4778\n",
      "Epoch  8 0.4993\n",
      "Epoch  9 0.5172\n",
      "Epoch  10 0.5354\n",
      "Epoch  11 0.5492\n",
      "Epoch  12 0.5648\n",
      "Epoch  13 0.5745\n",
      "Epoch  14 0.5874\n",
      "Epoch  15 0.5969\n",
      "Epoch  16 0.6064\n",
      "Epoch  17 0.6169\n",
      "Epoch  18 0.6252\n",
      "Epoch  19 0.6342\n",
      "Epoch  20 0.6387\n",
      "Epoch  21 0.6449\n",
      "Epoch  22 0.6527\n",
      "Epoch  23 0.6579\n",
      "Epoch  24 0.6654\n",
      "Epoch  25 0.6689\n",
      "Epoch  26 0.6724\n",
      "Epoch  27 0.6813\n",
      "Epoch  28 0.6851\n",
      "Epoch  29 0.686\n",
      "Epoch  30 0.693\n",
      "Epoch  31 0.7\n",
      "Epoch  32 0.701\n",
      "Epoch  33 0.7049\n",
      "Epoch  34 0.7046\n",
      "Epoch  35 0.7083\n",
      "Epoch  36 0.7115\n",
      "Epoch  37 0.7123\n",
      "Epoch  38 0.7182\n",
      "Epoch  39 0.722\n",
      "Epoch  40 0.7232\n",
      "Epoch  41 0.7273\n",
      "Epoch  42 0.7262\n",
      "Epoch  43 0.7298\n",
      "Epoch  44 0.7325\n",
      "Epoch  45 0.7374\n",
      "Epoch  46 0.738\n",
      "Epoch  47 0.7395\n",
      "Epoch  48 0.743\n",
      "Epoch  49 0.7433\n",
      "Epoch  50 0.7465\n",
      "Epoch  51 0.7466\n",
      "Epoch  52 0.7515\n",
      "Epoch  53 0.7501\n",
      "Epoch  54 0.7534\n",
      "Epoch  55 0.7543\n",
      "Epoch  56 0.7557\n",
      "Epoch  57 0.7576\n",
      "Epoch  58 0.7572\n",
      "Epoch  59 0.757\n",
      "Epoch  60 0.7601\n",
      "Epoch  61 0.7617\n",
      "Epoch  62 0.7649\n",
      "Epoch  63 0.7613\n",
      "Epoch  64 0.7669\n",
      "Epoch  65 0.7647\n",
      "Epoch  66 0.7642\n",
      "Epoch  67 0.7697\n",
      "Epoch  68 0.7711\n"
     ]
    }
   ],
   "source": [
    "resnet = CifarResNet()\n",
    "%time train_network(resnet, torch.device(DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Важно переключить сеть в режим eval - иначе dropout будет работать некорректно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_solution(a_net, a_device):\n",
    "    res = []\n",
    "    net = a_net.eval()\n",
    "    for item in dataloader_test_norm:\n",
    "        inputs = item.to(a_device)\n",
    "        outputs = net(inputs) \n",
    "\n",
    "        res += prediction2classes(outputs)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my_solution = make_solution(dense_net, DEVICE)\n",
    "my_solution = make_solution(resnet, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('my_solution.csv', 'w') as fout:\n",
    "    print('Id', 'Prediction', sep=',', file=fout)\n",
    "    for i, prediction in enumerate(my_solution):\n",
    "        print(i, prediction, sep=',', file=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {
    "82bf2faaba7c4ecaabb580ffd8565ea2": {
     "views": [
      {
       "cell_index": 19
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
